{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle competition\n",
    "\n",
    "Label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond\n",
    "\n",
    "\n",
    "#### What the data looks like ? \n",
    "Train-jpg: 40K+ satelite photos <br>\n",
    "train_v2: (image_name, tags) Unique tags 449 <br>\n",
    "So this is a multilabel image classification problem\n",
    "\n",
    "#### Challenge\n",
    "Unlike dog and cat image recognition, the image set are very similar in colors\n",
    "```terminal\n",
    "source activate tensorflow_p36\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi CUDA monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.integrate import cumtrapz\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math, glob, os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tags 449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_path = '/home/ubuntu/.kaggle/competitions/planet-understanding-the-amazon-from-space/'\n",
    "# kaggle_path = '/Users/mumuxi/.kaggle/competitions/planet-understanding-the-amazon-from-space/'\n",
    "labels = pd.read_csv(kaggle_path+'train_v2.csv')\n",
    "print ('Number of unique tags',len(labels['tags'].unique()))\n",
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validation():\n",
    "#     df = pd.read_csv(kaggle_path+'test_v2_file_mapping.csv')\n",
    "#     return df.head(3)\n",
    "# validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_photos(rows,columns):\n",
    "    fig=plt.figure(figsize=(15, 15))\n",
    "    for i in range(1, columns*rows +1):\n",
    "        path = kaggle_path+f'train-jpg/train_{i}.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        ax = fig.add_subplot(rows, columns, i)\n",
    "        ax.set_xlabel(labels.tags[i])\n",
    "        plt.imshow(img)\n",
    "# example_photos(rows=4,columns=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation - F2 SCORE\n",
    "F2 is used to evaluate how good the model is and has a formula given below\n",
    "\n",
    "$F_{2} = (1+β^{2})\\frac{pr}{β^{2}p+r}$\n",
    "\n",
    "\n",
    ",where *p* is precision, *r* is recall and β=2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tags EDA\n",
    "- Most tags contains primary\n",
    "- Clear has strong negative correlations with cloudy, haze, partly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = labels.tags.tolist()\n",
    "tokenizer = Tokenizer(filters = '\"#()*+,-./:;<=>?@[\\]^`{|}~')\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tags():\n",
    "    tags = tokenizer.word_docs\n",
    "    fig  = plt.figure(figsize=(10, 6))\n",
    "    plt.bar(tags.keys(), tags.values(), 1, color='lightblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Tags distribution')\n",
    "\n",
    "def img_multilabel(plot=True):\n",
    "    M  = tokenizer.texts_to_matrix(texts, mode='count')\n",
    "    df = pd.DataFrame(data=M,columns=['0']+list(tokenizer.word_index.keys()))\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[1:-1]\n",
    "    df = df[cols]\n",
    "    if plot:\n",
    "        corr = df.corr()\n",
    "        fig=plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr)\n",
    "    return df\n",
    "\n",
    "# plot_tags()\n",
    "df_label = img_multilabel(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haze vs clear  2697  :  28431\n"
     ]
    }
   ],
   "source": [
    "haze_id = df_label.loc[df_label['haze']==1].index.values\n",
    "clear_id = df_label.loc[df_label['clear']==1].index.values\n",
    "print ('haze vs clear ',len(haze_id),' : ',len(clear_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Image\n",
    "#### 1. Calculate haze degree\n",
    "Source: [Detecting Foggy Images and Estimating the Haze Degree Factor](https://www.omicsonline.org/open-access/detecting-foggy-images-and-estimating-the-haze-degree-factor-jcsb.1000226.pdf)\n",
    "\n",
    "Define haze degree as w, where w is a number between (0,1). Smaller w indicates a clearer image while haze images have greater w.  For example, w greater than 0.7 means 80%-100% of the image area is hazed.\n",
    "\n",
    "#### 2. Dehaze images\n",
    "Source: [Single Image Haze Removal Using Dark Channel Prior](https://www.robots.ox.ac.uk/~vgg/rg/papers/hazeremoval.pdf)\n",
    "\n",
    "The following formula is widely used to describe the formation of a haze image\n",
    "\n",
    "$I(x) = J(x)t(x) + A(1-t(x))$\n",
    "\n",
    "where *I* is the observed intensity, *J* represents the original haze-free image to be recovered, *A* is the global atmospheric light, and *t* is an exponential decay distribution .\n",
    "\n",
    "The aim is to recover *J* from *I*. We can achieve that by tuning *t*, as *t* has a formula of\n",
    "\n",
    "$t(x) = 1- w\\times min[min(\\frac{I_{c}(y)}{A^{c}})]$\n",
    ",where *w* is weight coefficient, *c* is the dark channel color (darkest channel of RGB channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Single image dehazing.\"\"\"\n",
    "class Channel_value:\n",
    "    val = -1.0\n",
    "    intensity = -1.0\n",
    "\n",
    "def find_intensity_of_atmospheric_light(img, gray):\n",
    "    \"\"\"return Atomospheric light A \"\"\"\n",
    "    # pick top 0.1% brightest pixels in the dark channel ~ most hazeopaque\n",
    "    top_num = int(img.shape[0] * img.shape[1] * 0.001)\n",
    "    toplist = [Channel_value()] * top_num \n",
    "    dark_channel = find_dark_channel(img)\n",
    "    for y in range(img.shape[0]):\n",
    "        for x in range(img.shape[1]):\n",
    "            val = img.item(y, x, dark_channel)\n",
    "            intensity = gray.item(y, x)\n",
    "            for t in toplist:  # create 65 different channels \n",
    "                if t.val < val or (t.val == val and t.intensity < intensity):\n",
    "                    t.val = val\n",
    "                    t.intensity = intensity\n",
    "                    break\n",
    "    max_channel = Channel_value()\n",
    "    for t in toplist:\n",
    "        if t.intensity > max_channel.intensity:\n",
    "            max_channel = t\n",
    "    return max_channel.intensity\n",
    "\n",
    "def find_dark_channel(img): #get darkest RBG channel\n",
    "    return np.unravel_index(np.argmin(img), img.shape)[2]\n",
    "\n",
    "def clamp(minimum, x, maximum):\n",
    "    return max(minimum, min(x, maximum))\n",
    "\n",
    "def dehaze_function(img, light_intensity, windowSize, t0, w):\n",
    "    size = (img.shape[0], img.shape[1])\n",
    "    outimg = np.zeros(img.shape, img.dtype)\n",
    "    for y in range(size[0]):\n",
    "        for x in range(size[1]):\n",
    "            x_low = max(x-(windowSize//2), 0)\n",
    "            y_low = max(y-(windowSize//2), 0)\n",
    "            x_high = min(x+(windowSize//2), size[1])\n",
    "            y_high = min(y+(windowSize//2), size[0])\n",
    "            sliceimg = img[y_low:y_high, x_low:x_high]\n",
    "            dark_channel = find_dark_channel(sliceimg)\n",
    "            t = 1.0 - (w * img.item(y, x, dark_channel) / light_intensity)\n",
    "            outimg.itemset((y,x,0), clamp(0, ((img.item(y,x,0) - light_intensity) / max(t, t0) + light_intensity), 255))\n",
    "            outimg.itemset((y,x,1), clamp(0, ((img.item(y,x,1) - light_intensity) / max(t, t0) + light_intensity), 255))\n",
    "            outimg.itemset((y,x,2), clamp(0, ((img.item(y,x,2) - light_intensity) / max(t, t0) + light_intensity), 255))\n",
    "    return outimg\n",
    "\n",
    "\n",
    "def dehaze_image(path,w = 0.95, t0 = 0.55):\n",
    "    img = cv2.imread(path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #image to gray scale\n",
    "    light_intensity = find_intensity_of_atmospheric_light(img, gray)\n",
    "    outimg = dehaze_function(img, light_intensity, 20, t0, w)\n",
    "    return outimg\n",
    "\n",
    "def example_photo_no(image_id):\n",
    "    path = kaggle_path+f'train-jpg/train_{image_id}.jpg'\n",
    "    img = cv2.imread(path)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "def intensity_plot(path):\n",
    "    img = cv2.imread(path)\n",
    "    color = ('b','g','r')\n",
    "    histr = cv2.calcHist([img],[0],None,[256],[0,256])\n",
    "    plt.plot(histr)\n",
    "## %%%%%%%%      GRB PLOT   %%%%%%%%\n",
    "#     for i,col in enumerate(color):\n",
    "#         histr = cv2.calcHist([img],[i],None,[256],[0,256])\n",
    "#         plt.plot(histr,color = col)\n",
    "#         plt.xlim([0,256])\n",
    "# ##  %%%%%%%     CDF plot    %%%%%%%%\n",
    "#     values, bins = np.histogram(img.ravel(),256,[0,256],normed=True)\n",
    "#     cum = np.cumsum(values)\n",
    "#     plt.plot(bins[:-1],cum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "miu, v, sigma = 5.1,2.9,.2461\n",
    "def calculate_haziness(image_id):\n",
    "    path = kaggle_path+f'train-jpg/train_{image_id}.jpg'\n",
    "    img = cv2.imread(path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #image to gray scale\n",
    "    A = find_intensity_of_atmospheric_light(img, gray)\n",
    "    D = img.min(2).sum()/(img.shape[0]*img.shape[1])\n",
    "    B = img.max(2).sum()/(img.shape[0]*img.shape[1])\n",
    "    C = B - D\n",
    "    w = math.e**(-0.5*(miu*(A-D)/A+v*C/A))\n",
    "    return (image_id,w)\n",
    "\n",
    "def compare_photos(image_id):\n",
    "    path = kaggle_path+f'train-jpg/train_{image_id}.jpg'\n",
    "    fig=plt.figure(figsize=(5, 8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    dehazed_img = dehaze_image(path)\n",
    "    plt.imshow(dehazed_img)\n",
    "    fig.add_subplot(212,sharex=ax1)\n",
    "    example_photo_no(image_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing.dummy import Pool as ThreadPool \n",
    "# pool = ThreadPool(10) \n",
    "# N = 1000\n",
    "# w = map(calculate_haziness,range(N))\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "# df_w = pd.DataFrame(data = w, columns = ['image_id','w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.4792106739524277)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_haziness(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('top picture is haze-free image, bottom one is the original picture')\n",
    "compare_photos(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "- Distiguish haze | clear label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Neural Network vanilla model\n",
    "- Importing the Keras libraries and packages\n",
    "- Sequential:<br> initialise model as a sequential network. There are two basic ways of initialising a neural network, either by a sequence of layers or as a graph\n",
    "\n",
    "#### 1.1 Load Data\n",
    "\n",
    "#### 1.2 Model Work Flow\n",
    "1. **Conv2D** \n",
    "    - Images => 2 D arrays\n",
    "    - Videos => Convolution 3-D \n",
    "    \n",
    "    \n",
    "2. **MaxPooling2D**:<br> The primary aim of a pooling operation is to reduce the size of the images. Different types of pooling operations like Min Pooling, Mean Pooling, etc. Here in MaxPooling we need the maximum value pixel from the respective region of interest.\n",
    "\n",
    "\n",
    "3. **Flatten**:<br> Flattening is the process of converting all the resultant 2 dimensional arrays into a continuous vector.\n",
    "\n",
    "\n",
    "4. **Dense** \n",
    "    - units: no nodes in the hidden layer\n",
    "5. **compile**\n",
    "    - Optimizer:  stochastic gradient descent algorithm\n",
    "    - Loss: loss function\n",
    "    - Metrics: performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(train_ids,test_ids):\n",
    "    read = lambda i: cv2.imread(kaggle_path+f'train-jpg/train_{i}.jpg')\n",
    "    train_array = np.array(list(map(read,train_ids)))\n",
    "    test_array = np.array(list(map(read,test_ids)))\n",
    "    return train_array,test_array\n",
    "\n",
    "def load_data():\n",
    "    y_train, y_test = train_test_split(df_label, test_size=0.3,random_state=50)\n",
    "    X_train, X_test = load_train_test(y_train.index.tolist(),y_test.index.tolist())\n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test = load_data()\n",
    "print (X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F2_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return (1+4)*precision*recall/(4*precision+recall)\n",
    "\n",
    "def build_layers():\n",
    "    \"\"\"32 filters of size 12x12\n",
    "        input image size 256 x 256, 3 for RGB\n",
    "    \"\"\"\n",
    "    no_features = 17\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(12,12), activation = 'relu',input_shape = (256, 256, 3)))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = no_features, activation = 'sigmoid'))\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    model = build_layers()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', \n",
    "                  metrics = ['accuracy',F2_score])\n",
    "    return model\n",
    "\n",
    "def train_model():\n",
    "    model = get_model()\n",
    "    model.fit(X_train, y_train, epochs=3, batch_size=200)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test loss: %.3f  accuracy: %.3f F2 score: %.3f.' %(score[0],score[1],score[2]))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "28335/28335 [==============================] - 96s 3ms/step - loss: 3.0949 - acc: 0.8055 - F2_score: 0.4511\n",
      "Epoch 2/3\n",
      "28335/28335 [==============================] - 92s 3ms/step - loss: 3.0684 - acc: 0.8087 - F2_score: 0.4521\n",
      "Epoch 3/3\n",
      "28335/28335 [==============================] - 92s 3ms/step - loss: 3.0684 - acc: 0.8087 - F2_score: 0.4521\n",
      "Test loss: 3.067  accuracy: 0.809 F2 score: 0.451.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f60b274ae48>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Pre-processing image\n",
    "The directory name is taken as the label of all the images present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.838  accuracy: 0.822 F2 score: 0.623.\n",
      "Val loss: 2.845  accuracy: 0.822 F2 score: 0.621.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
